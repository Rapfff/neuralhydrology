{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d168d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spotpy\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from superflexpy.implementation.root_finders.pegasus import PegasusPython\n",
    "from superflexpy.implementation.numerical_approximators.implicit_euler import ImplicitEulerPython\n",
    "from superflexpy.implementation.elements.hymod import LinearReservoir\n",
    "from superflexpy.implementation.elements.gr4j import ProductionStore\n",
    "from superflexpy.implementation.elements.structure_elements import  Splitter, Junction\n",
    "from superflexpy.framework.unit import Unit\n",
    "from superflexpy.framework.element import ParameterizedElement\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "train_start = '01/10/2006'\n",
    "train_end   = '30/09/2008'\n",
    "basin = \"01333000\"\n",
    "\n",
    "def load_data(basin,train_start,train_end):\n",
    "    p = load_p(basin,train_start,train_end)\n",
    "    pet = load_pet(basin,train_start,train_end)\n",
    "    data = np.vstack((p,pet)).T\n",
    "    return data\n",
    "def load_p(basin,train_start,train_end):\n",
    "    train_start_dt = datetime.strptime(train_start, '%d/%m/%Y')\n",
    "    train_end = datetime.strptime(train_end, '%d/%m/%Y')\n",
    "    len_seq = (train_end - train_start_dt).days + 1\n",
    "    for i in range(1, 13):\n",
    "        i = '0' * (1-(i // 10)) + str(i)\n",
    "        try:\n",
    "            f = open(\"data/CAMELS_US/basin_mean_forcing/daymet/\"+i+\"/\" + basin + \"_lump_cida_forcing_leap.txt\")\n",
    "            break\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    f.readline()\n",
    "    f.readline()\n",
    "    f.readline()\n",
    "    f.readline()\n",
    "    l = f.readline().split('\\t')\n",
    "    x = np.zeros(len_seq)\n",
    "    train_start = train_start.split('/')\n",
    "    train_start.reverse()\n",
    "    train_start = ' '.join(train_start)\n",
    "    while l[0] != train_start+' 12':\n",
    "        l = f.readline().split('\\t')\n",
    "    for i in range(len_seq):\n",
    "        x[i] = float(l[2])\n",
    "        l = f.readline().split('\\t')\n",
    "    return x\n",
    "\n",
    "def load_pet(basin,train_start,train_end):\n",
    "    with open(\"data/pet.pkl\", \"rb\") as fp:\n",
    "        results = pickle.load(fp)\n",
    "        results = results[basin]\n",
    "        results.index = pd.to_datetime(results.index)\n",
    "        train_start = datetime.strptime(train_start, '%d/%m/%Y')\n",
    "        train_end = datetime.strptime(train_end, '%d/%m/%Y')\n",
    "        results = results.loc[train_start:train_end]\n",
    "        results = results['PET(mm/d)'].values\n",
    "        results = np.maximum(0.0,results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae4a1ac",
   "metadata": {},
   "source": [
    "# 1 - Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cbda53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_area(basin):\n",
    "    for i in range(1,13):\n",
    "        i = '0' * (1-(i // 10)) + str(i)\n",
    "        try:\n",
    "            fp =  open(\"data/CAMELS_US/basin_mean_forcing/daymet/\"+i+\"/\"+basin+\"_lump_cida_forcing_leap.txt\", 'r')\n",
    "            # load area from header\n",
    "            fp.readline()\n",
    "            fp.readline()\n",
    "            area = int(fp.readline())\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    return area\n",
    "\n",
    "def load_qobs(basin,train_start,train_end):\n",
    "    area = get_area(basin)\n",
    "    train_start_dt = datetime.strptime(train_start, '%d/%m/%Y')\n",
    "    train_end = datetime.strptime(train_end, '%d/%m/%Y')\n",
    "    len_seq = (train_end - train_start_dt).days + 1\n",
    "    x = np.zeros(len_seq)\n",
    "    train_start = train_start.split('/')\n",
    "    train_start.reverse()\n",
    "    train_start = ' '.join(train_start)\n",
    "    for i in range(1, 13):\n",
    "        i = '0' * (1 - (i // 10)) + str(i)\n",
    "        try:\n",
    "            f = open(\"data/CAMELS_US/usgs_streamflow/\"+i+\"/\" + basin + \"_streamflow_qc.txt\")\n",
    "            break\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    l = f.readline()[9:].split(\" \")\n",
    "    while ' '.join([l[i] for i in range(3)]) != train_start:\n",
    "        l = f.readline()[9:].split(' ')\n",
    "    for i in range(len_seq):\n",
    "        x[i] = float(l[-2])\n",
    "        l = f.readline()[9:].split(' ')\n",
    "    x = 28316846.592 * x * 86400 / (area * 10**6)\n",
    "    return x\n",
    "\n",
    "P    = load_p(basin, train_start, train_end)\n",
    "Ep   = load_pet(basin, train_start, train_end)\n",
    "Qobs = load_qobs(basin, train_start, train_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9aa362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "for basin in [\"02027500\" ,\"03026500\",\"05487980\",\"03186500\",\"08175000\"]:\n",
    "    P = load_p(basin, '01/10/2001', '30/09/2008')\n",
    "    Ep = load_pet(basin, '01/10/2001', '30/09/2008')\n",
    "    Qobs = load_qobs(basin, '01/10/2001', '30/09/2008')\n",
    "    x = pd.DataFrame({\n",
    "        'P': P,\n",
    "        'Ep': Ep,\n",
    "        'Qobs': Qobs,\n",
    "        'dates': pd.date_range(start='2001-10-01', periods=len(P))\n",
    "    })\n",
    "    x.to_csv(basin+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc9b6e",
   "metadata": {},
   "source": [
    "# 2 - Superflex model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6067cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterizedSingleFluxSplitter(ParameterizedElement):\n",
    "    _num_downstream = 2\n",
    "    _num_upstream = 1\n",
    "    \n",
    "    def set_input(self, input):\n",
    "\n",
    "        self.input = {'Q_in': input[0]}\n",
    "\n",
    "    def get_output(self, solve=True):\n",
    "\n",
    "        split_par = self._parameters[self._prefix_parameters + 'split-par']\n",
    "\n",
    "        output1 = [self.input['Q_in'] * split_par]\n",
    "        output2 = [self.input['Q_in'] * (1 - split_par)]\n",
    "        \n",
    "        return [output1, output2]   \n",
    "    \n",
    "\n",
    "def create_superflex_model():\n",
    "    # Initialize numercal routines\n",
    "    root_finder = PegasusPython()\n",
    "    numeric_approximator = ImplicitEulerPython(root_finder=root_finder)\n",
    "\n",
    "    # Initialize the elements\n",
    "    lr_1 = LinearReservoir(\n",
    "        parameters={'k': 0.5},\n",
    "        states={'S0': 0.0},\n",
    "        approximation=numeric_approximator,\n",
    "        id='LR-1'\n",
    "    )\n",
    "\n",
    "    lr_2 = LinearReservoir(\n",
    "        parameters={'k': 4.0},\n",
    "        states={'S0': 0.0},\n",
    "        approximation=numeric_approximator,\n",
    "        id='LR-2'\n",
    "    )\n",
    "\n",
    "    tr = ProductionStore(\n",
    "        parameters={'x1': 20.0, 'alpha': 2.0,\n",
    "                    'beta': 2.0, 'ni': 0.0},\n",
    "        states={'S0': 0.0},\n",
    "        approximation=numeric_approximator,\n",
    "        id='TR'\n",
    "    )\n",
    "    splitter = ParameterizedSingleFluxSplitter(\n",
    "                        parameters={'split-par': 0.9},\n",
    "                        id='spl'\n",
    "    )\n",
    "\n",
    "    junction = Junction(direction=[[0, 0]], # Third output\n",
    "                        id='jun')\n",
    "\n",
    "    model = Unit(layers=[[tr],\n",
    "                        [splitter],\n",
    "                        [lr_1, lr_2],\n",
    "                        [junction]],\n",
    "                id='model')\n",
    "    return model\n",
    "\n",
    "sf_model = create_superflex_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e373f679",
   "metadata": {},
   "source": [
    "# 3 - Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ddadb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_fun_nse(obs, sim):\n",
    "    idx = (~np.isnan(sim)) & (~np.isnan(obs))\n",
    "    obs = obs[idx]\n",
    "    sim = sim[idx]\n",
    "    denominator = ((obs - obs.mean())**2).sum()\n",
    "    numerator = ((sim - obs)**2).sum()\n",
    "    value = numerator / denominator\n",
    "    return float(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714fc6e4",
   "metadata": {},
   "source": [
    "# 4 - Spotpy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b15ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "class spotpy_model(object):\n",
    "\n",
    "    def __init__(self, model, inputs, dt, observations, parameters, parameter_names, output_index, \n",
    "                 warm_up = 365):\n",
    "\n",
    "        self._model = model\n",
    "        self._model.set_input(inputs)\n",
    "        self._model.set_timestep(dt)\n",
    "\n",
    "        self._parameters = parameters\n",
    "        self._parameter_names = parameter_names\n",
    "        self._observarions = observations\n",
    "        self._output_index = output_index\n",
    "        \n",
    "        self._warm_up = int(warm_up)\n",
    "        \n",
    "    def parameters(self):\n",
    "        return spotpy.parameter.generate(self._parameters)\n",
    "    \n",
    "    def simulation(self, parameters):\n",
    "\n",
    "        named_parameters = {}\n",
    "        for p_name, p in zip(self._parameter_names, parameters):\n",
    "            named_parameters[p_name] = p\n",
    "\n",
    "        self._model.set_parameters(named_parameters)\n",
    "        self._model.reset_states()\n",
    "        output = self._model.get_output()\n",
    "\n",
    "        return output[self._output_index]\n",
    "    \n",
    "    def evaluation(self):\n",
    "        return self._observarions\n",
    "    \n",
    "    # Here you can use a pre-defined objective function from spotpy, or you can write down your own:    \n",
    "    def objectivefunction(self, simulation, evaluation):        \n",
    "        evaluation_used = evaluation[self._warm_up + 1:]\n",
    "        simulation_used = simulation[self._warm_up + 1:]\n",
    "        obj_fun = obj_fun_nse(evaluation_used, simulation_used)\n",
    "        \n",
    "        return obj_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b231c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spotpy_sf = spotpy_model(\n",
    "    model=sf_model,\n",
    "    inputs=[Ep,P],\n",
    "    dt=1.0,\n",
    "    observations = Qobs,\n",
    "    parameters=[\n",
    "        spotpy.parameter.Uniform('model_TR_x1', 1.0, 500.0),\n",
    "        spotpy.parameter.Uniform('model_LR-1_k', 0.01, 50.0),\n",
    "        spotpy.parameter.Uniform('model_LR-2_k', 0.01, 50.0),\n",
    "        spotpy.parameter.Uniform('model_spl_split-par', 0.0, 1.0)\n",
    "    ],\n",
    "    parameter_names=['model_TR_x1','model_LR-1_k','model_LR-2_k','model_spl_split-par'],\n",
    "    output_index=0,\n",
    "    warm_up=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f222c208",
   "metadata": {},
   "source": [
    "# 5 - Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba93206",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = spotpy.algorithms.sceua(spotpy_sf, dbname='calibration', dbformat='csv')\n",
    "sampler.sample(repetitions=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716e4feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_names = ['TR_smax','LR1_k','LR2_k','Splitter1','Splitter2']\n",
    "sf_model_parameters = [208.061, 19.2237, 0.112066, 0.112686] #[377.757, 24.4519, 0.144759, 0.179659]\n",
    "sf_model_parameters.append(1-sf_model_parameters[-1])\n",
    "if sf_model_parameters[1]<sf_model_parameters[2]:\n",
    "    sf_model_parameters[1], sf_model_parameters[2] = sf_model_parameters[2],sf_model_parameters[1]\n",
    "    sf_model_parameters[3], sf_model_parameters[4] = sf_model_parameters[4],sf_model_parameters[3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf8195c",
   "metadata": {},
   "source": [
    "# 6 - Running NH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3511a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralhydrology.nh_run import start_run\n",
    "from pathlib import Path\n",
    "#for basin in ['03026500', \"01333000\", \"02027500\"]:#, \"05487980\", \"03186500\", \"08175000\"]:\n",
    "#    with open(\"examples/07-Superflex/1_basin_list.txt\",'w') as f:\n",
    "#        f.write(basin)\n",
    "#    f1 =  open(\"examples/07-Superflex/experiment.yml\", 'w')\n",
    "#    f2 =  open(\"examples/07-Superflex/example4.yml\", 'r')\n",
    "#    l = f2.readline()\n",
    "#    while l[:7] != 'run_dir':\n",
    "#        f1.write(l)\n",
    "#        l = f2.readline()\n",
    "#    f1.write('run_dir: runs/' + basin + '\\n')\n",
    "#    l = f2.readline()\n",
    "#    while l:\n",
    "#        f1.write(l)\n",
    "#        l = f2.readline()\n",
    "#    f1.close()\n",
    "#    f2.close()\n",
    "#    for _ in range(5):\n",
    "#        start_run(config_file=Path(\"examples/07-Superflex/experiment.yml\"))\n",
    "start_run(config_file=Path(\"examples/07-Superflex/example4.yml\", gpu=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7898c7d8",
   "metadata": {},
   "source": [
    "# 7 - Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488e1294",
   "metadata": {},
   "source": [
    "# 7.0 - Analysis tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34cb043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from os import listdir\n",
    "from torch import load, nn, tensor, float32\n",
    "from neuralhydrology.datasetzoo.camelsus import load_camels_us_attributes\n",
    "from superflexpy.implementation.root_finders.pegasus import PegasusPython\n",
    "from superflexpy.implementation.numerical_approximators.implicit_euler import ImplicitEulerPython\n",
    "from superflexpy.implementation.elements.hymod import LinearReservoir\n",
    "from superflexpy.implementation.elements.gr4j import ProductionStore\n",
    "from superflexpy.implementation.elements.structure_elements import Splitter, Junction\n",
    "from superflexpy.framework.unit import Unit\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def load_p(basin, train_start, train_end):\n",
    "    train_start_dt = datetime.strptime(train_start, '%d/%m/%Y')\n",
    "    train_end = datetime.strptime(train_end, '%d/%m/%Y')\n",
    "    len_seq = (train_end - train_start_dt).days + 1\n",
    "    for i in range(1, 13):\n",
    "        i = '0' * (1 - (i // 10)) + str(i)\n",
    "        try:\n",
    "            f = open(\"data/CAMELS_US/basin_mean_forcing/daymet/\" + i + \"/\" + basin + \"_lump_cida_forcing_leap.txt\")\n",
    "            break\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    f.readline()\n",
    "    f.readline()\n",
    "    f.readline()\n",
    "    f.readline()\n",
    "    l = f.readline().split('\\t')\n",
    "    x = np.zeros(len_seq)\n",
    "    train_start = train_start.split('/')\n",
    "    train_start.reverse()\n",
    "    train_start = ' '.join(train_start)\n",
    "    while l[0] != train_start + ' 12':\n",
    "        l = f.readline().split('\\t')\n",
    "    for i in range(len_seq):\n",
    "        x[i] = float(l[2])\n",
    "        l = f.readline().split('\\t')\n",
    "    return x\n",
    "\n",
    "def load_pet(basin, train_start, train_end):\n",
    "    with open(\"data/pet.pkl\", \"rb\") as fp:\n",
    "        results = pickle.load(fp)\n",
    "        results = results[basin]\n",
    "        results.index = pd.to_datetime(results.index)\n",
    "        train_start = datetime.strptime(train_start, '%d/%m/%Y')\n",
    "        train_end = datetime.strptime(train_end, '%d/%m/%Y')\n",
    "        results = results.loc[train_start:train_end]\n",
    "        results = results['PET(mm/d)'].values\n",
    "        results = np.maximum(0.0, results)\n",
    "    return results\n",
    "\n",
    "def epoch_int_to_str(nb_epochs):\n",
    "    nb_epochs = str(nb_epochs)\n",
    "    nb_epochs = (3 - len(nb_epochs)) * '0' + nb_epochs\n",
    "    return nb_epochs\n",
    "\n",
    "def to_xarray(arr, start_date, time_step_value=1):\n",
    "    start_date = datetime.strptime(start_date, '%d/%m/%Y')\n",
    "    start_date = start_date.strftime('%Y-%m-%d')\n",
    "    dates = pd.date_range(start=start_date, periods=len(arr))\n",
    "    return xr.DataArray(arr, coords={'date': dates, 'time_step': time_step_value}, dims=['date'])\n",
    "\n",
    "def get_nh_validation(run_dir, basins, first_epoch, last_epoch=None, epoch_interval=1):\n",
    "    \"\"\"return a dict : res[basin_str][epoch_int] -> iterable over timesteps\"\"\"\n",
    "    if last_epoch is None:\n",
    "        last_epoch = first_epoch\n",
    "        epoch_interval = 1\n",
    "    res = {}\n",
    "    for basin in basins:\n",
    "        res[basin] = {}\n",
    "        for epoch in range(first_epoch, last_epoch + 1, epoch_interval):\n",
    "            nb_epochs = epoch_int_to_str(epoch)\n",
    "            with open(run_dir + \"/validation/model_epoch\" + nb_epochs + \"/validation_results.p\", \"rb\") as fp:\n",
    "                results = pickle.load(fp)\n",
    "                qsim = results[basin]['1D']['xr']['QObs(mm/d)_sim'].squeeze().values\n",
    "                res[basin][epoch] = qsim\n",
    "    return res\n",
    "\n",
    "def get_nh_observed(run_dir, basins, first_epoch=1):\n",
    "    \"\"\"return a dict : res[basin_str] -> iterable over timesteps\"\"\"\n",
    "    res = {}\n",
    "    for basin in basins:\n",
    "        res[basin] = []\n",
    "        nb_epochs = epoch_int_to_str(first_epoch)\n",
    "        with open(run_dir + \"/validation/model_epoch\" + nb_epochs + \"/validation_results.p\", \"rb\") as fp:\n",
    "            results = pickle.load(fp)\n",
    "            qobs = results[basin]['1D']['xr']['QObs(mm/d)_obs'].squeeze().values\n",
    "            res[basin] = qobs\n",
    "    return res\n",
    "\n",
    "def get_nh_metrics(run_dir, basins, first_epoch, last_epoch=None, epoch_interval=1):\n",
    "    \"\"\"return a dict : res[basin_str][\"NSE\" or \"KGE\"] -> iterable over epochs\"\"\"\n",
    "    if last_epoch is None:\n",
    "        last_epoch = first_epoch\n",
    "        epoch_interval = 1\n",
    "    res = {}\n",
    "    for basin in basins:\n",
    "        res[basin] = {'NSE': [], 'KGE': []}\n",
    "        for nb_epochs in range(epoch_interval, last_epoch + 1, epoch_interval):\n",
    "            nb_epochs = epoch_int_to_str(nb_epochs)\n",
    "            with open(run_dir + \"/validation/model_epoch\" + nb_epochs + \"/validation_results.p\", \"rb\") as fp:\n",
    "                results = pickle.load(fp)\n",
    "                nse_nh = results[basin]['1D']['NSE']\n",
    "                kge_nh = results[basin]['1D']['KGE']\n",
    "            res[basin]['NSE'].append(nse_nh)\n",
    "            res[basin]['KGE'].append(kge_nh)\n",
    "    return res\n",
    "\n",
    "def scale_output(x, min_val, max_val):\n",
    "    scaled_x = x * (max_val - min_val) + min_val\n",
    "    return scaled_x.detach().item()\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        #torch.nn.Sequential(torch.nn.Linear(in_features=static_inputs_size, out_features=20),\n",
    "        #                    torch.nn.Sigmoid(), torch.nn.Dropout(p=0.0),\n",
    "        #                    torch.nn.Linear(in_features=20, out_features=total_parameters),\n",
    "        #                    torch.nn.Sigmoid())\n",
    "        super(MyModel, self).__init__()\n",
    "        self.parameterization = nn.Sequential(nn.Linear(in_features=3, out_features=20),\n",
    "                                              nn.Sigmoid(), nn.Dropout(p=0.0), nn.Linear(in_features=20,\n",
    "                                                                                         out_features=20), nn.Sigmoid(),\n",
    "                                              nn.Linear(in_features=20, out_features=5), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.parameterization(x)\n",
    "\n",
    "def get_nh_parameters(run_dir, basins, parameter_names, first_epoch, last_epoch=None, epoch_interval=1):\n",
    "    \"\"\"return a dict : res[basin_str][parameter_name_str] -> iterable over epochs\n",
    "    # WORKS for arch1 only, otherwise => change 'MyModel' class\"\"\"\n",
    "    if last_epoch is None:\n",
    "        last_epoch = first_epoch\n",
    "        epoch_interval = 1\n",
    "    res = {}\n",
    "    all_x_s = load_camels_us_attributes(\"data/CAMELS_US\", basins)\n",
    "    for ib, basin in enumerate(basins):\n",
    "        x_s = tensor(all_x_s.loc[basin, [\"baseflow_index\", \"high_q_freq\", \"aridity\"]].values.astype(float), dtype=float32)\n",
    "        res[basin] = {}\n",
    "        for pn in parameter_names:\n",
    "            res[basin][pn] = []\n",
    "        for nb_epochs in range(epoch_interval, last_epoch + 1, epoch_interval):\n",
    "            epoch_str = epoch_int_to_str(nb_epochs)\n",
    "            ordereddict = load(run_dir + '/model_epoch' + epoch_str + '.pt')\n",
    "            m = MyModel()\n",
    "            m.load_state_dict(ordereddict)\n",
    "            params_epoch = m(x_s)\n",
    "            if len(basins) > 1:\n",
    "                params_epoch = params_epoch[ib]\n",
    "            res[basin][\"TR_smax\"].append(scale_output(params_epoch[0], 1.0, 500.0))\n",
    "            r1, r2 = scale_output(params_epoch[3], 0.01, 50.0), scale_output(params_epoch[4], 0.01, 50.0)\n",
    "            weights = params_epoch[1:3]\n",
    "            row_sums = weights.sum()\n",
    "            normalized_weights = weights / row_sums\n",
    "            p1, p2 = normalized_weights[0].detach().item(), normalized_weights[1].detach().item()\n",
    "            if r1<r2:\n",
    "                r1, r2 = r2, r1\n",
    "                p1, p2 = p2, p1\n",
    "            res[basin][\"LR1_k\"  ].append(r1)\n",
    "            res[basin][\"LR2_k\"].append(r2)\n",
    "            res[basin][\"Splitter1\"].append(p1)\n",
    "            res[basin][\"Splitter2\"].append(p2)\n",
    "    return res\n",
    "\n",
    "def get_of_parameters(basin):\n",
    "    with open('runs/openFlex/demo/01/output_' + basin + '/par_Mraf_cal.dat') as f:\n",
    "        l = f.readline()\n",
    "        l = f.readline()\n",
    "        l.replace(' ', '')\n",
    "        smax_of = float(l[:-1])\n",
    "        l = f.readline()\n",
    "        l.replace(' ', '')\n",
    "        w = float(l[:-1])\n",
    "        l = f.readline()\n",
    "        l.replace(' ', '')\n",
    "        r1_of = float(l[:-1])\n",
    "        l = f.readline()\n",
    "        l.replace(' ', '')\n",
    "        r2_of = float(l[:-1])\n",
    "    if r1_of < r2_of:\n",
    "        r2_of, r1_of = r1_of, r2_of\n",
    "    else:\n",
    "        w = 1.0-w\n",
    "    return {\"TR_smax\": smax_of, 'LR1_k': r1_of, 'LR2_k': r2_of, \"Splitter1\": w, \"Splitter2\": 1.0-w}\n",
    "\n",
    "def get_of_nse(basin):\n",
    "    with open('runs/openFlex/demo/01/output_' + basin + '/par_Mraf_cal.dat') as f:\n",
    "        for _ in range(8):\n",
    "            l = f.readline()\n",
    "        l = l.replace(\" \", \"\")\n",
    "        l = l[l.index(\"=\") + 1:]\n",
    "        l = float(l)\n",
    "    return l\n",
    "\n",
    "def get_sf_validation(basin, validation_start, validation_end,sf_model_parameters, seq_length=365, predict_last_n=1):\n",
    "    root_finder = PegasusPython()\n",
    "    numeric_approximator = ImplicitEulerPython(root_finder=root_finder)\n",
    "    lr_1 = LinearReservoir(parameters={'k': sf_model_parameters[1]},\n",
    "                        states={'S0': 0.0},\n",
    "                        approximation=numeric_approximator,\n",
    "                        id='LR-1')\n",
    "    lr_2 = LinearReservoir(parameters={'k': sf_model_parameters[2]},\n",
    "                        states={'S0': 0.0},\n",
    "                        approximation=numeric_approximator,\n",
    "                        id='LR-2')\n",
    "    tr = ProductionStore(parameters={\n",
    "        'x1': sf_model_parameters[0],\n",
    "        'alpha': 2.0,\n",
    "        'beta': 2.0,\n",
    "        'ni': 0.0\n",
    "        },\n",
    "        states={'S0': 0.0},\n",
    "        approximation=numeric_approximator,\n",
    "        id='TR')\n",
    "    splitter = Splitter(weight=[[sf_model_parameters[3]], [1 - sf_model_parameters[3]]], direction=[[0], [0]], id='spl')\n",
    "\n",
    "    junction = Junction(\n",
    "        direction=[[0, 0]],  # Third output\n",
    "        id='jun')\n",
    "\n",
    "    model_sf = Unit(layers=[[tr], [splitter], [lr_1, lr_2], [junction]], id='model')\n",
    "\n",
    "    validation_start = datetime.strptime(validation_start, '%d/%m/%Y')\n",
    "    validation_start = validation_start - timedelta(days=seq_length - predict_last_n)\n",
    "    validation_start = validation_start.strftime('%d/%m/%Y')\n",
    "    #validation_end   = (datetime.strptime('01/10/2002', '%d/%m/%Y') + timedelta(days=1)).strftime('%d/%m/%Y')\n",
    "\n",
    "    P = load_p(basin, validation_start, validation_end)\n",
    "    Ep = load_pet(basin, validation_start, validation_end)\n",
    "    output_sf = []\n",
    "    for offset in range(0, len(Ep) - seq_length + 1, predict_last_n):\n",
    "        model_sf.reset_states()\n",
    "        model_sf.set_input([Ep[offset:offset + seq_length], P[offset:offset + seq_length]])\n",
    "        model_sf.set_timestep(1.0)\n",
    "        output_sf += model_sf.get_output()[0][-predict_last_n:].tolist()\n",
    "    output_sf = np.array(output_sf)\n",
    "    return output_sf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01b99f5",
   "metadata": {},
   "source": [
    "## 7.1 - Neuralhydrology outflow and metrics analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4434ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========SET THIS=================\n",
    "run_dir = \"runs/run_2304_110046\"\n",
    "first_epoch = 1\n",
    "last_epoch = 10\n",
    "epoch_interval = 1\n",
    "#basins = [\"01510000\",\n",
    "#          \"02027500\",\n",
    "#          \"03026500\",\n",
    "#          \"05487980\",\n",
    "#          \"06853800\",\n",
    "#          \"08175000\"]\n",
    "basins = ['01333000', '01333000']\n",
    "#------------------------------------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(len(basins), figsize=(20, 10))\n",
    "sim = get_nh_validation(run_dir, basins,first_epoch, last_epoch, epoch_interval)\n",
    "obs = get_nh_observed(run_dir, basins, first_epoch)\n",
    "for ib, basin in enumerate(basins):\n",
    "    for nb_epochs in range(1, last_epoch + 1, epoch_interval):\n",
    "        ax[ib].plot(sim[basin][nb_epochs],label='epoch '+str(nb_epochs))\n",
    "    ax[ib].plot(obs[basin], label='obs')\n",
    "    ax[ib].set_title(basin)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(len(basins), figsize=(20, 10))\n",
    "ax2 = [None for _ in basins]\n",
    "mets = get_nh_metrics(run_dir, basins, first_epoch, last_epoch, epoch_interval)\n",
    "\n",
    "for ib, basin in enumerate(basins):\n",
    "    ax[ib].set_xlabel('epochs')\n",
    "    ax[ib].set_ylabel('NSE', color='tab:red')\n",
    "    ax[ib].plot(range(first_epoch, last_epoch+1, epoch_interval),mets[basin]['NSE'],color='tab:red')\n",
    "    ax2[ib] = ax[ib].twinx()\n",
    "    ax2[ib].set_ylabel('KGE', color='tab:blue')\n",
    "    ax[ib].plot(range(first_epoch, last_epoch+1, epoch_interval),mets[basin]['KGE'],color='tab:blue')\n",
    "    ax[ib].set_title(basin)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9e4772",
   "metadata": {},
   "source": [
    "## 7.2 - Neuralhydrology parameters analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029768e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_names = ['TR_smax', 'LR1_k', 'LR2_k', 'Splitter1', 'Splitter2']\n",
    "nh_model_parameters = []\n",
    "basin = '03026500'\n",
    "for run_dir in listdir('runs/'+basin):\n",
    "    nh_model_parameters.append(get_nh_parameters('runs/'+basin+'/'+run_dir, [basin], parameter_names, first_epoch, last_epoch, epoch_interval))\n",
    "#nh_model_parameters.append(get_nh_parameters(run_dir, [basin], parameter_names, first_epoch, last_epoch, epoch_interval))\n",
    "fig, ax = plt.subplots(len(parameter_names), figsize=(20, 20))\n",
    "for ip, param in enumerate(parameter_names):\n",
    "    for r in range(5):\n",
    "        ax[ip].plot(range(first_epoch, last_epoch + 1, epoch_interval), nh_model_parameters[r][basin][param], label='run'+str(r))\n",
    "        ax[ip].set_title(param)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc92a52",
   "metadata": {},
   "source": [
    "## 7.3 - Comparison with Superflex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7beea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralhydrology.evaluation import metrics\n",
    "import seaborn as sns\n",
    "\n",
    "basin = \"01333000\"\n",
    "validation_start = '01/10/2006'\n",
    "validation_end   = '30/09/2008'\n",
    "warmup = 0\n",
    "last_epoch = 7\n",
    "\n",
    "output_sf = get_sf_validation(basin, validation_start, validation_end, [208.061, 19.2237, 0.112066, 0.112686])\n",
    "output_sf_xarray = to_xarray(output_sf, validation_start)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(20, 10))\n",
    "nse_nh = get_nh_metrics(run_dir, basins=[basin], first_epoch=last_epoch)[basin]['NSE'][-1]\n",
    "qobs =  get_nh_observed(run_dir, [basin], first_epoch)\n",
    "qobs_xarray = to_xarray(qobs[basin], validation_start)\n",
    "nse_sf = metrics.nse(qobs_xarray,output_sf_xarray)\n",
    "qsim = get_nh_validation(run_dir, basins=[basin], first_epoch=last_epoch)[basin][last_epoch]\n",
    "\n",
    "print(\"NSE NH:\",nse_nh, \"\\nNSE SF:\",nse_sf,'\\n')\n",
    "\n",
    "#for pnames,nh,sf in zip(parameter_names,nh_model_parameters,sf_model_parameters):\n",
    "#    print(pnames.ljust(10),'NH:',str(round(nh,5)).ljust(10),'SF:',str(round(sf,5)).ljust(10))\n",
    "\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.lineplot(data=pd.DataFrame({'obs':qobs[basin][warmup:],\n",
    "                                \"nh\": qsim[warmup:],\n",
    "                                \"sf\": output_sf[warmup:]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35afe28",
   "metadata": {},
   "source": [
    "# 8 - Batch comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4f4693",
   "metadata": {},
   "source": [
    "## 8.1 - Against Fortran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f43a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "parameter_names = ['TR_smax', 'LR1_k', 'LR2_k', 'Splitter1', 'Splitter2']\n",
    "# Load SF parameters\n",
    "for basin in ['03026500', '01333000','02027500']:\n",
    "    algo = []\n",
    "    data = {'NSE':[], 'TR_smax':[], 'LR1_k':[], 'LR2_k':[], 'Splitter1':[]}\n",
    "    with open('runs/openFlex/demo/01/output_'+basin+'/par_Mraf_cal.dat') as f:\n",
    "        algo.append(\"OF\")\n",
    "        data['NSE'].append(get_of_nse(basin))\n",
    "        params_of = get_of_parameters(basin)\n",
    "        data['TR_smax'].append(params_of['TR_smax'])\n",
    "        data['LR1_k'].append(params_of['LR1_k'])\n",
    "        data['LR2_k'].append(params_of['LR2_k'])\n",
    "        data['Splitter1'].append(params_of['Splitter1'])\n",
    "\n",
    "    nb_epochs = 10\n",
    "    for run_dir in listdir('runs/'+basin):\n",
    "        algo.append(\"NH\")\n",
    "        data['NSE'].append(get_nh_metrics('runs/'+basin+'/'+run_dir,[basin],nb_epochs)[basin]['NSE'][-1])\n",
    "        params_nh = get_nh_parameters('runs/'+basin+'/'+run_dir, [basin], parameter_names,10)\n",
    "        data['TR_smax'].append(params_nh[basin]['TR_smax'][-1])\n",
    "        data['LR1_k'].append(params_nh[basin]['LR1_k'][-1])\n",
    "        data['LR2_k'].append(params_nh[basin]['LR2_k'][-1])\n",
    "        data['Splitter1'].append(params_nh[basin]['Splitter1'][-1])\n",
    "    sns.set_theme(style=\"ticks\", palette=\"pastel\")\n",
    "    f, axes = plt.subplots(1, len(parameter_names),figsize=(20, 10))\n",
    "    for i,n in enumerate(['NSE'] + parameter_names[:-1]):\n",
    "        x = pd.DataFrame({\n",
    "            'value': data[n],\n",
    "            'algo': algo\n",
    "        })\n",
    "        # Draw a nested boxplot to show bills by day and time\n",
    "        sns.boxplot(x=[n]*len(algo),\n",
    "                    y=\"value\",\n",
    "                    hue=\"algo\",\n",
    "                    data=x,\n",
    "                    ax=axes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c413260",
   "metadata": {},
   "source": [
    "Comparing forward pass for models with exactly the same parameter values:\n",
    "- 1. Fortran\n",
    "- 2. Neuralhydrology\n",
    "- 3. A simulated model\n",
    "- 4. Superflex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1850722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralhydrology.nh_run import start_run\n",
    "from neuralhydrology.modelzoo import Superflex\n",
    "from neuralhydrology.utils.config import Config\n",
    "from neuralhydrology.evaluation import metrics\n",
    "\n",
    "from pathlib import Path\n",
    "from torch import tensor\n",
    "from numpy import reshape, dstack\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. - Neuralhydrology --------------------------------------\n",
    "cfg = Config(Path('examples/07-Superflex/example4.yml'))\n",
    "\n",
    "m = Superflex(cfg)\n",
    "\n",
    "p = load_p('01333000','01/10/2005', '30/09/2008')\n",
    "l = len(p)\n",
    "p = reshape(p, (1, l, 1))\n",
    "ep = load_pet('01333000', '01/10/2005', '30/09/2008')\n",
    "ep = reshape(ep, (1, len(ep), 1))\n",
    "xd = dstack((p,ep))\n",
    "qobs = load_qobs('01333000', '01/10/2005', '30/09/2008')\n",
    "qobs = reshape(qobs,(1,len(qobs),1))\n",
    "dates = pd.date_range(start='2005-10-01', periods=l)\n",
    "\n",
    "x_s = tensor([[0.7204, 0.5716, 4.5000]])\n",
    "per_basin_target_stds = tensor([[[1.5724]]])\n",
    "\n",
    "length = cfg.seq_length\n",
    "\n",
    "nh_output= []\n",
    "\n",
    "for i in range(length,l):\n",
    "    if int(100*(i-length)/(l-length)) % 10 == 0:\n",
    "        print(int(100 * (i - length) / (l - length)), '%',end='\\r')\n",
    "    x_d = tensor(xd[: , i - length:i, :])\n",
    "    y = tensor(qobs[i-length:i])\n",
    "    date = pd.date_range(start=dates[i-length], periods=length)\n",
    "    data = {\n",
    "        'x_s': x_s,\n",
    "        'y': y,\n",
    "        'date': date,\n",
    "        'x_d': x_d,\n",
    "        'per_basin_target_stds' : per_basin_target_stds\n",
    "    }\n",
    "    o = m(data)['y_hat']\n",
    "    o = o[-1][-1][-1].item()\n",
    "    nh_output.append(o)\n",
    "print()\n",
    "\n",
    "# 2. - Frotran --------------------------------------\n",
    "fr_output = []\n",
    "with open('runs/openFlex/demo/01/output_'+'01333000'+'/res_Mraf.dat') as f:\n",
    "    for i in range(369):\n",
    "        l = f.readline()\n",
    "    while l:\n",
    "        fr_output.append(float(l.split('    ')[6]))\n",
    "        l = f.readline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eeaf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ------------------------------------------------\n",
    "f, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.plot(fr_output[:-1], alpha=0.7, label='openflex')\n",
    "ax.plot(nh_output[1:] , alpha=0.7, label='neuralhydrology')\n",
    "#ax.plot(output_sf, '--', alpha=0.5, label='sf')\n",
    "#ax.plot(sim, '--', alpha=0.5, label='sim')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
