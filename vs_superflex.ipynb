{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d168d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spotpy\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from superflexpy.implementation.root_finders.pegasus import PegasusPython\n",
    "from superflexpy.implementation.numerical_approximators.implicit_euler import ImplicitEulerPython\n",
    "from superflexpy.implementation.elements.hymod import LinearReservoir\n",
    "from superflexpy.implementation.elements.gr4j import ProductionStore\n",
    "from superflexpy.implementation.elements.structure_elements import  Splitter, Junction\n",
    "from superflexpy.framework.unit import Unit\n",
    "from superflexpy.framework.element import ParameterizedElement\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "train_start = '01/10/2006'\n",
    "train_end   = '30/09/2008'\n",
    "basin = \"01350080\"\n",
    "\n",
    "def load_data(basin,train_start,train_end):\n",
    "    p = load_p(basin,train_start,train_end)\n",
    "    pet = load_pet(basin,train_start,train_end)\n",
    "    data = np.vstack((p,pet)).T\n",
    "    return data\n",
    "def load_p(basin,train_start,train_end):\n",
    "    train_start_dt = datetime.strptime(train_start, '%d/%m/%Y')\n",
    "    train_end = datetime.strptime(train_end, '%d/%m/%Y')\n",
    "    len_seq = (train_end - train_start_dt).days\n",
    "    f = open(\"data/CAMELS_US/basin_mean_forcing/daymet/02/\"+basin+\"_lump_cida_forcing_leap.txt\")\n",
    "    f.readline()\n",
    "    f.readline()\n",
    "    f.readline()\n",
    "    f.readline()\n",
    "    l = f.readline().split('\\t')\n",
    "    x = np.zeros(len_seq)\n",
    "    train_start = train_start.split('/')\n",
    "    train_start.reverse()\n",
    "    train_start = ' '.join(train_start)\n",
    "    while l[0] != train_start+' 12':\n",
    "        l = f.readline().split('\\t')\n",
    "    for i in range(len_seq):\n",
    "        x[i] = float(l[2])\n",
    "        l = f.readline().split('\\t')\n",
    "    return x\n",
    "\n",
    "def load_pet(basin,train_start,train_end):\n",
    "    with open(\"data\\pet.pkl\", \"rb\") as fp:\n",
    "        results = pickle.load(fp)\n",
    "        results = results[basin]\n",
    "        results.index = pd.to_datetime(results.index)\n",
    "        train_start = datetime.strptime(train_start, '%d/%m/%Y')\n",
    "        train_end = datetime.strptime(train_end, '%d/%m/%Y') - timedelta(days=1)\n",
    "        results = results.loc[train_start:train_end]\n",
    "        results = results['PET(mm/d)'].values\n",
    "        results = np.maximum(0.0,results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae4a1ac",
   "metadata": {},
   "source": [
    "# 1 - Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cbda53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_area(basin):\n",
    "    with open(\"data/CAMELS_US/basin_mean_forcing/daymet/02/\"+basin+\"_lump_cida_forcing_leap.txt\", 'r') as fp:\n",
    "        # load area from header\n",
    "        fp.readline()\n",
    "        fp.readline()\n",
    "        area = int(fp.readline())\n",
    "    return area\n",
    "\n",
    "def load_qobs(basin,train_start,train_end):\n",
    "    area = get_area(basin)\n",
    "    train_start_dt = datetime.strptime(train_start, '%d/%m/%Y')\n",
    "    train_end = datetime.strptime(train_end, '%d/%m/%Y')\n",
    "    len_seq = (train_end - train_start_dt).days\n",
    "    x = np.zeros(len_seq)\n",
    "    train_start = train_start.split('/')\n",
    "    train_start.reverse()\n",
    "    train_start = ' '.join(train_start)\n",
    "    f = open(\"data/CAMELS_US/usgs_streamflow/02/\"+basin+\"_streamflow_qc.txt\")\n",
    "    l = f.readline()[9:].split(\" \")\n",
    "    while ' '.join([l[i] for i in range(3)]) != train_start:\n",
    "        l = f.readline()[9:].split(' ')\n",
    "    for i in range(len_seq):\n",
    "        x[i] = float(l[-2])\n",
    "        l = f.readline()[9:].split(' ')\n",
    "    x = 28316846.592 * x * 86400 / (area * 10**6)\n",
    "    return x\n",
    "    \n",
    "P    = load_p(basin, train_start, train_end)\n",
    "Ep   = load_pet(basin, train_start, train_end)\n",
    "Qobs = load_qobs(basin, train_start, train_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc9b6e",
   "metadata": {},
   "source": [
    "# 2 - Superflex model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6067cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterizedSingleFluxSplitter(ParameterizedElement):\n",
    "    _num_downstream = 2\n",
    "    _num_upstream = 1\n",
    "    \n",
    "    def set_input(self, input):\n",
    "\n",
    "        self.input = {'Q_in': input[0]}\n",
    "\n",
    "    def get_output(self, solve=True):\n",
    "\n",
    "        split_par = self._parameters[self._prefix_parameters + 'split-par']\n",
    "\n",
    "        output1 = [self.input['Q_in'] * split_par]\n",
    "        output2 = [self.input['Q_in'] * (1 - split_par)]\n",
    "        \n",
    "        return [output1, output2]   \n",
    "    \n",
    "\n",
    "def create_superflex_model():\n",
    "    # Initialize numercal routines\n",
    "    root_finder = PegasusPython()\n",
    "    numeric_approximator = ImplicitEulerPython(root_finder=root_finder)\n",
    "\n",
    "    # Initialize the elements\n",
    "    lr_1 = LinearReservoir(\n",
    "        parameters={'k': 0.5},\n",
    "        states={'S0': 0.0},\n",
    "        approximation=numeric_approximator,\n",
    "        id='LR-1'\n",
    "    )\n",
    "\n",
    "    lr_2 = LinearReservoir(\n",
    "        parameters={'k': 4.0},\n",
    "        states={'S0': 0.0},\n",
    "        approximation=numeric_approximator,\n",
    "        id='LR-2'\n",
    "    )\n",
    "\n",
    "    tr = ProductionStore(\n",
    "        parameters={'x1': 20.0, 'alpha': 2.0,\n",
    "                    'beta': 2.0, 'ni': 0.0},\n",
    "        states={'S0': 0.0},\n",
    "        approximation=numeric_approximator,\n",
    "        id='TR'\n",
    "    )\n",
    "    splitter = ParameterizedSingleFluxSplitter(\n",
    "                        parameters={'split-par': 0.9},\n",
    "                        id='spl'\n",
    "    )\n",
    "\n",
    "    junction = Junction(direction=[[0, 0]], # Third output\n",
    "                        id='jun')\n",
    "\n",
    "    model = Unit(layers=[[tr],\n",
    "                        [splitter],\n",
    "                        [lr_1, lr_2],\n",
    "                        [junction]],\n",
    "                id='model')\n",
    "    return model\n",
    "\n",
    "sf_model = create_superflex_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e373f679",
   "metadata": {},
   "source": [
    "# 3 - Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ddadb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_fun_nsee(observations, simulation, expo = 0.5):\n",
    "    metric = np.nansum(np.power(np.power(simulation, expo)  - np.power(observations,expo), 2)) / np.nansum(np.power(np.power(observations, expo)  - np.nanmean(np.power(observations, expo)), 2))\n",
    "    return float(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714fc6e4",
   "metadata": {},
   "source": [
    "# 4 - Spotpy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b15ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "class spotpy_model(object):\n",
    "\n",
    "    def __init__(self, model, inputs, dt, observations, parameters, parameter_names, output_index, \n",
    "                 warm_up = 365):\n",
    "\n",
    "        self._model = model\n",
    "        self._model.set_input(inputs)\n",
    "        self._model.set_timestep(dt)\n",
    "\n",
    "        self._parameters = parameters\n",
    "        self._parameter_names = parameter_names\n",
    "        self._observarions = observations\n",
    "        self._output_index = output_index\n",
    "        \n",
    "        self._warm_up = int(warm_up)\n",
    "        \n",
    "    def parameters(self):\n",
    "        return spotpy.parameter.generate(self._parameters)\n",
    "    \n",
    "    def simulation(self, parameters):\n",
    "\n",
    "        named_parameters = {}\n",
    "        for p_name, p in zip(self._parameter_names, parameters):\n",
    "            named_parameters[p_name] = p\n",
    "\n",
    "        self._model.set_parameters(named_parameters)\n",
    "        self._model.reset_states()\n",
    "        output = self._model.get_output()\n",
    "\n",
    "        return output[self._output_index]\n",
    "    \n",
    "    def evaluation(self):\n",
    "        return self._observarions\n",
    "    \n",
    "    # Here you can use a pre-defined objective function from spotpy, or you can write down your own:    \n",
    "    def objectivefunction(self, simulation, evaluation):        \n",
    "        evaluation_used = evaluation[self._warm_up + 1:]\n",
    "        simulation_used = simulation[self._warm_up + 1:]\n",
    "        obj_fun = obj_fun_nsee(observations = evaluation_used, simulation = simulation_used, expo = 0.5)\n",
    "        \n",
    "        return obj_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b231c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spotpy_sf = spotpy_model(\n",
    "    model=sf_model,\n",
    "    inputs=[Ep,P],\n",
    "    dt=1.0,\n",
    "    observations = Qobs,\n",
    "    parameters=[\n",
    "        spotpy.parameter.Uniform('model_TR_x1', 1.0, 500.0),\n",
    "        spotpy.parameter.Uniform('model_LR-1_k', 0.01, 50.0),\n",
    "        spotpy.parameter.Uniform('model_LR-2_k', 0.01, 50.0),\n",
    "        spotpy.parameter.Uniform('model_spl_split-par', 0.0, 1.0)\n",
    "    ],\n",
    "    parameter_names=['model_TR_x1','model_LR-1_k','model_LR-2_k','model_spl_split-par'],\n",
    "    output_index=0,\n",
    "    warm_up=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f222c208",
   "metadata": {},
   "source": [
    "# 5 - Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba93206",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = spotpy.algorithms.sceua(spotpy_sf, dbname='calibration', dbformat='csv')\n",
    "sampler.sample(repetitions=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf8195c",
   "metadata": {},
   "source": [
    "# 6 - Running NH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3511a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralhydrology.nh_run import start_run\n",
    "from pathlib import Path\n",
    "start_run(config_file=Path(\"examples/07-Superflex/example4.yml\", gpu=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a40846",
   "metadata": {},
   "source": [
    "# 7 - Results\n",
    "\n",
    "## 7.1 - Neuralhydrology outflow analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4434ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========SET THIS=================\n",
    "run_dir = \"runs/run_2803_173209\"\n",
    "epoch_max = 15\n",
    "validation_interval = 3\n",
    "basins = [\"01510000\",\n",
    "          \"02027500\",\n",
    "          \"03026500\",\n",
    "          \"05487980\",\n",
    "          \"06853800\",\n",
    "          \"08175000\"]\n",
    "#basins = [\"01333000\",'01333000']\n",
    "#------------------------------------\n",
    "\n",
    "def epoch_int_to_str(nb_epochs):\n",
    "    nb_epochs = str(nb_epochs)\n",
    "    nb_epochs = (3-len(nb_epochs))*'0'+nb_epochs\n",
    "    return nb_epochs\n",
    "\n",
    "\n",
    "basins = basins\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(len(basins), figsize=(20, 20))\n",
    "for ib, basin in enumerate(basins):\n",
    "    for nb_epochs in range(validation_interval,epoch_max+1,validation_interval):\n",
    "        nb_epochs = epoch_int_to_str(nb_epochs)\n",
    "        with open(run_dir + \"/validation/model_epoch\"+nb_epochs+\"/validation_results.p\", \"rb\") as fp:\n",
    "            results = pickle.load(fp)\n",
    "            qsim = results[basin]['1D']['xr']['QObs(mm/d)_sim'].squeeze().values\n",
    "            qobs = results[basin]['1D']['xr']['QObs(mm/d)_obs']\n",
    "            ax[ib].plot(qsim,label='epoch'+nb_epochs)\n",
    "    ax[ib].plot(qobs.squeeze().values,'--',label='obs')\n",
    "    ax[ib].set_title(basin)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9e4772",
   "metadata": {},
   "source": [
    "## 7.2 - Neuralhydrology parameters analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029768e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import load,nn, tensor, float32\n",
    "from neuralhydrology.datasetzoo.camelsus import load_camels_us_attributes\n",
    "#===========SET THIS=================\n",
    "#run_dir = \"runs/run_2203_133730\"\n",
    "#epoch_max = 10\n",
    "#validation_interval = 2\n",
    "#basins = [\"01510000\",\n",
    "#          \"02027500\",\n",
    "#          \"03026500\",\n",
    "#          \"05487980\",\n",
    "#          \"06853800\",\n",
    "#          \"08175000\"]\n",
    "#------------------------------------\n",
    "\n",
    "def scale_output(x, min_val, max_val):\n",
    "    scaled_x = x * (max_val - min_val) + min_val\n",
    "    return scaled_x.detach().item()\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        #torch.nn.Sequential(torch.nn.Linear(in_features=static_inputs_size, out_features=20),\n",
    "        #                    torch.nn.Sigmoid(), torch.nn.Dropout(p=0.0),\n",
    "        #                    torch.nn.Linear(in_features=20, out_features=total_parameters),\n",
    "        #                    torch.nn.Sigmoid())\n",
    "        super(MyModel, self).__init__()\n",
    "        self.parameterization = nn.Sequential(nn.Linear(in_features=3, out_features=20),\n",
    "                                                    nn.Sigmoid(), nn.Dropout(p=0.0),\n",
    "                                                    nn.Linear(in_features=20, out_features=20),\n",
    "                                                    nn.Sigmoid(),\n",
    "                                                    nn.Linear(in_features=20, out_features=5),\n",
    "                                                    nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.parameterization(x)\n",
    "\n",
    "def get_parameters_values(run_dir,epoch_str,basin):\n",
    "    ordereddict = load(run_dir+'/model_epoch'+epoch_str+'.pt')\n",
    "    m = MyModel()\n",
    "    m.load_state_dict(ordereddict)\n",
    "    x_s = load_camels_us_attributes(\"data/CAMELS_US\",basins)\n",
    "    x_s = tensor(x_s.loc[basin,[\"baseflow_index\", \"high_q_freq\", \"aridity\"]].values.astype(float),dtype=float32)\n",
    "    return m(x_s)\n",
    "\n",
    "parameter_names = [\"TR_smax\",\"Splitter1\",\"Splitter2\",\"RR1_rate\",\"RR2_rate\"]\n",
    "parameters = [[[] for j in basins] for i in parameter_names]\n",
    "for ib, basin in enumerate(basins):\n",
    "    for nb_epochs in range(validation_interval,epoch_max+1,validation_interval):\n",
    "        nb_epochs = epoch_int_to_str(nb_epochs)\n",
    "        params_epoch = get_parameters_values(run_dir, nb_epochs, basin)\n",
    "        parameters[0][ib].append(scale_output(params_epoch[0], 1.0, 500.0))\n",
    "        weights = params_epoch[1:3]\n",
    "        row_sums = weights.sum()\n",
    "        normalized_weights = weights / row_sums\n",
    "        parameters[1][ib].append(normalized_weights[0].detach().item())\n",
    "        parameters[2][ib].append(normalized_weights[1].detach().item())\n",
    "        parameters[3][ib].append(scale_output(params_epoch[3], 0.01, 50.0))\n",
    "        parameters[4][ib].append(scale_output(params_epoch[4], 0.01, 50.0))\n",
    "        #parameters[1][ib].append(scale_output(params_epoch[1], 0.01, 50.0))\n",
    "\n",
    "fig, ax = plt.subplots(len(parameter_names), figsize=(20, 20))\n",
    "for ip, param in enumerate(parameter_names):\n",
    "    for ib, basin in enumerate(basins):\n",
    "        ax[ip].plot(parameters[ip][ib],label=basin)\n",
    "    ax[ip].set_title(param)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc92a52",
   "metadata": {},
   "source": [
    "## 7.3 - Comparison with Superflex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7beea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralhydrology.evaluation import metrics\n",
    "basin = \"01350080\"\n",
    "nb_epochs = 15\n",
    "root_finder = PegasusPython()\n",
    "numeric_approximator = ImplicitEulerPython(root_finder=root_finder)\n",
    "lr_1 = LinearReservoir(\n",
    "    parameters={'k': 19.8616},#0.0336838\n",
    "    states={'S0': 0.0},\n",
    "    approximation=numeric_approximator,\n",
    "    id='LR-1'\n",
    ")\n",
    "lr_2 = LinearReservoir(\n",
    "    parameters={'k': 0.136237},#18.2155\n",
    "    states={'S0': 0.0},\n",
    "    approximation=numeric_approximator,\n",
    "    id='LR-2'\n",
    ")\n",
    "tr = ProductionStore(\n",
    "    parameters={'x1': 30.2143, 'alpha': 2.0, #34.9774\n",
    "                'beta': 2.0, 'ni': 0.0},\n",
    "    states={'S0': 0.0},\n",
    "    approximation=numeric_approximator,\n",
    "    id='TR'\n",
    ")\n",
    "splitter = Splitter(weight=[[0.0756081], [1-0.0756081]], #0.635073\n",
    "                direction=[[0], [0]],\n",
    "                id='spl')\n",
    "\n",
    "junction = Junction(direction=[[0, 0]], # Third output\n",
    "                    id='jun')\n",
    "\n",
    "model_sf = Unit(layers=[[tr],\n",
    "                    [splitter],\n",
    "                    [lr_1, lr_2],\n",
    "                    [junction]],\n",
    "            id='model')\n",
    "\n",
    "P = load_p(basin, '01/10/2002','30/09/2004')\n",
    "Ep = load_pet(basin, '01/10/2002','30/09/2004')\n",
    "\n",
    "model_sf.reset_states()\n",
    "model_sf.set_input([Ep,P])\n",
    "model_sf.set_timestep(1.0)\n",
    "output_sf = model_sf.get_output()[0]\n",
    "\n",
    "\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "def mean(l):\n",
    "    return sum(l)/len(l)\n",
    "\n",
    "def stdev(l):\n",
    "    m = mean(l)\n",
    "    return sqrt(sum([(i-m)**2 for i in l])/len(l))\n",
    "\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1,1, figsize=(20, 10))\n",
    "nb_epochs = str(nb_epochs)\n",
    "nb_epochs = (3-len(nb_epochs))*'0'+nb_epochs\n",
    "with open(\"runs/run_2803_121443\" + \"/validation/model_epoch\"+nb_epochs+\"/validation_results.p\", \"rb\") as fp:\n",
    "    results = pickle.load(fp)\n",
    "    qsim = results[basin]['1D']['xr']['QObs(mm/d)_sim']\n",
    "    qobs = results[basin]['1D']['xr']['QObs(mm/d)_obs']\n",
    "    nse_nh = metrics.nse(qobs.isel(time_step=-1),qsim.isel(time_step=-1))\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "warmup = 0\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "dates = pd.date_range(start='2002-10-01', periods=len(output_sf))\n",
    "time_step_value = 0  # Assuming a constant time_step value\n",
    "# Create an xarray DataArray with the array values, coordinates, and dimensions\n",
    "output_sf_xarray = xr.DataArray(output_sf, coords={'date': dates, 'time_step': time_step_value},\n",
    "                                dims=['date'])\n",
    "nse_sf = metrics.nse(qobs.isel(time_step=-1)[:-1],output_sf_xarray)\n",
    "\n",
    "print(\"NSE NH:\",nse_nh, \"\\nNSE SF:\",nse_sf)\n",
    "sns.lineplot(data=pd.DataFrame({#'observed':qobs.squeeze()[warmup+1:],\n",
    "                                \"nh\": qsim.squeeze()[warmup+1:],\n",
    "                                \"sf\": output_sf[warmup:]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068cc4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show results for training period => C:\\Users\\reynoura\\Pictures\\exp\\04_04\\sf_vs_nh_training.png\n",
    "# Compare SF & NH according to NSE => NSE NH: 0.396800696849823 NSE SF: 0.2530828306385584\n",
    "# NH: 2 catchments one hot encoding vs SF: for each catchment => done\n",
    "# try with Alberto static inputs\n",
    "# try Martin tips"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
